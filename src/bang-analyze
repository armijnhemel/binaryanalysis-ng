#!/usr/bin/env python3

# Binary Analysis Next Generation (BANG!)
#
# This file is part of BANG.
#
# BANG is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License,
# version 3, as published by the Free Software Foundation.
#
# BANG is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public
# License, version 3, along with BANG.  If not, see
# <http://www.gnu.org/licenses/>
#
# Copyright 2018-2021 - Armijn Hemel
# Licensed under the terms of the GNU Affero General Public License
# version 3
# SPDX-License-Identifier: AGPL-3.0-only
#
# Gets a file and unpacks contents using standard functionality in
# Python 3 or some custom code and writes the contents to a temporary
# directory.

import sys
import os
import pathlib
import stat
import shutil
import datetime
import tempfile
import uuid

# import modules needed for multiprocessing
import multiprocessing
import queue

# import some module for collecting statistics and information about
# the run time environment of the tool, plus of runs.
import logging

# import module for database
import psycopg2

# import module for Elasticsearch
try:
     # ugly hack to work around import issues on Fedora 33
    import elasticsearch
except:
    pass

# import other local files
from banganalyzeroptions import BangAnalyzerOptions
from banglogging import log
import banglogging

from reporter.picklereport import *
from reporter.jsonreport import *
from reporter.humanreadablereport import *

# ugly hack to work around import issues on Fedora 33
if 'elasticsearch' in sys.modules:
    from reporter.elasticsearchreport import *

from FileContentsComputer import *
from FileResult import FileResult
from AnalysisEnvironment import *
from AnalysisJob import *
from PickleReader import *

def connect_to_bang_database(options):
    return psycopg2.connect(database=options.postgresql_db,
                            user=options.postgresql_user,
                            password=options.postgresql_password,
                            port=options.postgresql_port,
                            host=options.postgresql_host)


def main(argv):
    options = BangAnalyzerOptions().get()

    if not os.path.isdir(options.checkpath):
        print("Error: path should be a BANG scan results directory",
              file=sys.stderr)
        sys.exit(1)

    scandirectory = pathlib.Path(options.checkpath)
    # The directory structure inside the scandirectory:
    # unpack/ -- this is where all the unpacked data have been stored
    # results/ -- this is where files describing the unpacked data
    #             have been stored
    unpackdirectory = scandirectory / "unpack"

    resultsdirectory = scandirectory / "results"

    resultspickle = scandirectory / 'bang.pickle'

    # check if the directory is a valid BANG scan results directory
    if not resultspickle.exists():
        print("Error: path should be a BANG scan results directory",
              file=sys.stderr)
        sys.exit(1)

    if not unpackdirectory.exists():
        print("Error: path should be a BANG scan results directory",
              file=sys.stderr)
        sys.exit(1)
    if not resultsdirectory.exists():
        print("Error: path should be a BANG scan results directory",
              file=sys.stderr)
        sys.exit(1)

    # test the database. If the connection fails it depends on whether
    # or not it should be treated as a fatal error. If not, then
    # continue without using any of the database functionality.
    if options.usedatabase:
        try:
            conn = connect_to_bang_database(options)
            conn.close()
        except Exception as ex:
            if options.postgresql_error_fatal:
                print("Database error: missing/wrong configuration or database not running",
                      file=sys.stderr)
                sys.exit(1)
            options.usedatabase = False

    # create separate analysis directory
    if options.createjson or options.writereport:
        pass

    # tuple of database connection/database cursor
    bangdbconns = []

    # create database connections if any database is used
    if options.usedatabase:
        for i in range(0, options.bangthreads):
            bangconn = connect_to_bang_database(options)
            bangcursor = bangconn.cursor()
            bangdbconns.append((bangconn, bangcursor))

    # test if Elasticsearch is running
    if options.elastic_enabled:
        # ugly hack to work around import issues on Fedora 33
        if 'elasticsearch' not in sys.modules:
            options.elastic_enabled = False

    if options.uselogging:
        banglogging.uselogging = True
        # set up logging
        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(message)s')
        banglogger = logging.getLogger()

        # remove the standard log handler, as it will be different
        # per scan directory
        for i in banglogger.handlers:
            banglogger.removeHandler(i)

    # store a UTC time stamp
    analysis_date = datetime.datetime.utcnow()

    # create a unique identifier for the analysis
    analysis_uuid = uuid.uuid4()

    # create a process manager for managing the threads
    processmanager = multiprocessing.Manager()

    # first create a queues for analyzing files
    scan_queue = processmanager.JoinableQueue(maxsize=0)
    processes = []

    # create a lock to control access to any shared data structures
    processlock = multiprocessing.Lock()

    # create a shared dictionary
    checksumdict = processmanager.dict()

    # create an analysis environment for the new analysis
    scanenvironment = AnalysisEnvironment(
        runfilescans = options.runfilescans,
        logging = banglogging.uselogging,
        scan_queue = scan_queue,
        processlock = processlock,
        checksumdict = checksumdict,
        )
    #scanenvironment.set_unpackparsers(bangsignatures.get_unpackers())

    # read results from a Python pickle
    picklefile = open(scandirectory / 'bang.pickle', 'rb')
    scanresult = PickleReader(scanenvironment).top_level_read(picklefile)
    picklefile.close()

    # first report data in various formats
    if options.createjson:
        jsonfile = open(scandirectory / 'bang.json', 'w')
        JsonReporter(jsonfile).report(scanresult)
        jsonfile.close()

    # optionally create a human readable report of the scan results
    if options.writereport:
        reportfile = open(scandirectory / 'report.txt', 'w')
        HumanReadableReporter(reportfile).report(scanresult)
        reportfile.close()

    # optionally store existing data in Elasticsearch
    if options.elastic_enabled:
        ElasticsearchReporter(options).report(scanresult)

    # create processes for analyzing files
    for i in range(0, options.bangthreads):
        if not options.usedatabase:
            dbconn0 = None
            dbconn1 = None
        else:
            dbconn0 = bangdbconns[i][0]
            dbconn1 = bangdbconns[i][1]
        process = multiprocessing.Process(
            target=processfile,
            args=(dbconn0, dbconn1, scanenvironment))
        processes.append(process)

    # Turn the pickle back into FileResult objects and
    # put them into the scan queue
    root_element = None
    for fileresult in scanresult['scantree']:
        if 'root' in scanresult['scantree'][fileresult]['labels']:
            root_element = scanresult['scantree'][fileresult]
        #j = AnalysisJob(FileResult(scanresult['scantree'][fileresult]))
        #scan_queue.put(j)

    # then start all the processes
    for process in processes:
        process.start()

    # wait for the queues to be empty.
    scan_queue.join()

    # Done processing, terminate processes that were created
    for process in processes:
        process.terminate()

    log(logging.INFO, "Finished scanning %s" % checkfile)

    if options.uselogging:
        # flush any remaining data to the log file
        bangloghandler.flush()

        # remove the log file from the system logger
        banglogger.removeHandler(bangloghandler)
        bangloghandler.close()

    # clean up the database cursors and
    # close all connections to the database
    for ccs in bangdbconns:
        # first close the cursor
        ccs[1].close()
        # then close the database connection
        ccs[0].close()

    # finally shut down logging
    logging.shutdown()


if __name__ == "__main__":
    main(sys.argv)
