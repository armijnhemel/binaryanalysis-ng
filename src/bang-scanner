#!/usr/bin/python3

# Binary Analysis Next Generation (BANG!)
#
# This file is part of BANG.
#
# BANG is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License,
# version 3, as published by the Free Software Foundation.
#
# BANG is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public
# License, version 3, along with BANG.  If not, see
# <http://www.gnu.org/licenses/>
#
# Copyright 2018 - Armijn Hemel
# Licensed under the terms of the GNU Affero General Public License
# version 3
# SPDX-License-Identifier: AGPL-3.0-only
#
# Gets a file and unpacks contents using standard functionality in
# Python 3 or some custom code and writes the contents to a temporary
# directory.

import sys
import os
import pathlib
import stat
import shutil
import argparse
import configparser
import datetime
import tempfile
import re
import hashlib
import string
import math
import pickle
import json
import copy
import subprocess
import inspect
import collections
import functools
import mimetypes

# import module for database
import psycopg2

# import modules needed for multiprocessing
import multiprocessing
import queue

# import some module for collecting statistics and information about
# the run time environment of the tool, plus of runs.
import logging
import platform
import getpass

# import the local file with unpacking methods
import bangunpack
import bangsignatures
import bangfilescans

# For proximity matching use the TLSH module. This is not a standard
# module.
import tlsh

# store the maximum look ahead window. This is unlikely to matter, but
# just in case.
maxsignaturelength = max(map(lambda x: len(x), bangsignatures.signatures.values()))
maxsignaturesoffset = max(bangsignatures.signaturesoffset.values()) + maxsignaturelength


# Process a single file.
# This method has the following parameters:
#
# * scanfilequeue :: a queue where files to scan will be fetched from
# * resultqueue :: a queue where results will be written to
# * processlock :: a lock object that guards access to shared objects
# * checksumdict :: a shared dictionary to store hashes of files so
#   unnecessary scans of duplicate files can be prevented.
# * unpackdirectory :: the absolute path of the top level directory in
#   which files will be unpacked
# * resultsdirectory :: the absolute path of the directory where results
#   will be written to
# * temporary directory :: the absolute path of a directory in which
#   temporary files will be written
# * dbconn :: a PostgreSQL database connection
# * dbcursor :: a PostgreSQL database cursor
# * bangfilefunctions :: a list of functions for individual files
# * scanenvironment :: a dict that describes the environment in
#   which the scans run
#
# Each file will be in the scan queue and have the following data
# associated with it:
#
# * file name :: absolute path to the file to be scanned
# * set of labels :: either empty or containing hints from unpacking
# * parent :: name of parent file)
# * extradata :: empty, reserved for future use
#
# For every file a set of labels describing the file (such as 'binary' or
# 'graphics') will be stored. These labels can be used to feed extra
# information to the unpacking process, such as preventing scans from
# running.
def processfile(scanfilequeue, resultqueue, processlock, checksumdict,
                unpackdirectory, resultsdirectory, temporarydirectory,
                dbconn, dbcursor, bangfilefunctions, scanenvironment):
    lenunpackdirectory = len(str(unpackdirectory)) + 1
    synthesizedminimum = 10

    emptyhashes = {}
    emptyhashresults = {}

    # pre-store empty hashes
    for hashtocompute in ['sha256', 'md5', 'sha1']:
        emptyhashes[hashtocompute] = hashlib.new(hashtocompute)
        emptyhashresults[hashtocompute] = emptyhashes[hashtocompute].hexdigest()

    readsize = scanenvironment['readsize']
    createbytecounter = scanenvironment['createbytecounter']
    tlshmaximum = scanenvironment['tlshmaximum']

    while True:
        # grab a new file from the scanning queue
        (checkfile, labels, parent, extradata) = scanfilequeue.get(timeout=86400)

        # store the results of the file
        # At minimum store:
        # * file name (relative to the top level unpack directory))
        # * labels
        fileresult = {'fullfilename': str(checkfile)}
        if 'root' not in labels:
            fileresult['parent'] = str(parent)[lenunpackdirectory:]
        fileresult['filename'] = str(checkfile)[lenunpackdirectory:]

        # First perform all kinds of checks to prevent the file being
        # scanned.

        # Check if the file is a symbolic link.
        if checkfile.is_symlink():
            labels.append('symbolic link')
            fileresult['labels'] = labels
            resultqueue.put(fileresult)
            scanfilequeue.task_done()
            continue

        # Check if the file is a socket
        if stat.S_ISSOCK(os.stat(checkfile).st_mode):
            labels.append('socket')
            fileresult['labels'] = labels
            resultqueue.put(fileresult)
            scanfilequeue.task_done()
            continue

        # Check if the file is a FIFO
        if stat.S_ISFIFO(os.stat(checkfile).st_mode):
            labels.append('fifo')
            fileresult['labels'] = labels
            resultqueue.put(fileresult)
            scanfilequeue.task_done()
            continue

        # Check if the file is a block device
        if stat.S_ISBLK(os.stat(checkfile).st_mode):
            labels.append('block device')
            fileresult['labels'] = labels
            resultqueue.put(fileresult)
            scanfilequeue.task_done()
            continue

        # Check if the file is a character device
        if stat.S_ISCHR(os.stat(checkfile).st_mode):
            labels.append('character device')
            fileresult['labels'] = labels
            resultqueue.put(fileresult)
            scanfilequeue.task_done()
            continue

        # Check if the file is a directory
        if checkfile.is_dir():
            labels.append('directory')
            fileresult['labels'] = labels
            resultqueue.put(fileresult)
            scanfilequeue.task_done()
            continue

        filesize = os.stat(checkfile).st_size

        # Don't scan an empty file
        if filesize == 0:
            labels.append('empty')
            fileresult['labels'] = labels
            fileresult['filesize'] = 0
            resultqueue.put(fileresult)
            scanfilequeue.task_done()
            for i in emptyhashresults:
                fileresult[i] = emptyhashresults[i]
            continue

        # create an empty list of files that were
        # unpacked/carved from this file
        fileresult['unpackedfiles'] = []

        # store the last known position in the file with successfully
        # unpacked data
        lastunpackedoffset = -1

        # remove any duplicate labels
        labels = list(set(labels))

        needsunpacking = True

        # keep a byte range of data that has already been looked at
        unpackedrange = []

        # check for a dummy value to see if the file has already been
        # unpacked and if so, simply report and skip the unpacking, and
        # move on to just running the per file scans.
        if 'unpacked' in labels:
            labels.remove('unpacked')
            if 'rescan' in labels:
                origunpackedrange = [(0, filesize)]
                unpackedrange = [(0, 1)]
                lastunpackedoffset = 1
            else:
                needsunpacking = False
                lastunpackedoffset = filesize
                unpackedrange = [(0, filesize)]

                # store lot of information about the unpacked files
                # TODO: add more information, such as signature
                report = {}
                report['offset'] = 0
                report['size'] = filesize
                report['files'] = []
                fileresult['unpackedfiles'].append(report)

        # compute various checksums of the file
        hashresults = {}
        hashes = {}

        usetlsh = False
        if filesize >= 256 and filesize <= tlshmaximum:
            usetlsh = True

        # not all files with TLSH should be looked at
        tlshlabelsignore = set(['compressed', 'graphics', 'audio', 'filesystem', 'srec', 'ihex'])
        if set(labels).intersection(tlshlabelsignore) != set():
            usetlsh = False

        for hashtocompute in ['sha256', 'md5', 'sha1']:
            hashes[hashtocompute] = hashlib.new(hashtocompute)

        if createbytecounter:
            bytecounter = collections.Counter()

        # keep track of whether or not a file consists of just
        # ASCII characters. Initially assume it is text only.
        istext = True

        # open the file and seek to the start
        scanfile = open(checkfile, 'rb')
        scanfile.seek(0)

        if usetlsh:
            # initialize a TLSH hash, even though it might
            # not be used in the end.
            tlshhash = tlsh.Tlsh()

            # read all the data. Unfortunately TLSH currently does
            # not support Python's buffer protocol so this is a bit
            # inefficient.
            hashingdata = scanfile.read(readsize)

            while hashingdata != b'':
                # check if any of the characters read is a
                # non-ASCII character. If so, it is not a text file.
                if istext:
                    if len(list(filter(lambda x: chr(x) not in string.printable, hashingdata))) != 0:
                        istext = False

                if createbytecounter:
                    bytecounter.update(hashingdata)
                for h in hashes:
                    hashes[h].update(hashingdata)
                tlshhash.update(hashingdata)
                hashingdata = scanfile.read(readsize)
            tlshhash.final()
            hashes['tlsh'] = tlshhash
        else:
            # use Python's buffer protocol
            scanbytesarray = bytearray(readsize)
            bytesread = scanfile.readinto(scanbytesarray)
            hashingdata = memoryview(scanbytesarray[:bytesread])
            while bytesread != 0:
                if istext:
                    if len(list(filter(lambda x: chr(x) not in string.printable, hashingdata))) != 0:
                        istext = False
                if createbytecounter:
                    bytecounter.update(hashingdata)
                for h in hashes:
                    hashes[h].update(hashingdata)

                bytesread = scanfile.readinto(scanbytesarray)
                hashingdata = memoryview(scanbytesarray[:bytesread])

        scanfile.close()

        for f in hashes:
            try:
                hashresults[f] = hashes[f].hexdigest()
            except:
                pass

        for i in hashresults:
            fileresult[i] = hashresults[i]

        # store if files are text or binary
        if istext:
            labels.append('text')
        else:
            labels.append('binary')

        # Search the extension of the file in a list of known extensions.
        # https://www.iana.org/assignments/media-types/media-types.xhtml

        mimeres = mimetypes.guess_type(checkfile.name)
        if mimeres[0] is not None:
            fileresult['mimetype'] = mimeres[0]
            if mimeres[1] is not None:
                fileresult['mimetype encoding'] = mimeres[1]

        # Now try to unpack the file. There are a few categories:
        #
        # 1. a known extension, without a known magic header. This is
        #    for for example Android sparse data image formats, or
        #    several other Android or Google formats (Chrome PAK, etc.)
        #
        # 2. blobs, searching for known magic headers and carving them
        #    from blobs, or regular files. The majority of files will
        #    likely be processed here.
        #
        # 3. text only files, where it is not immediately clear what
        #    is inside and where the file possibly first has to be
        #    converted to a binary (examples: Intel Hex).

        # first process files with a known extension, but where
        # there is no clear magic header. A prime example is the
        # Android sparse data image format (system.new.dat and friends)
        for e in bangsignatures.extensiontofunction:
            if checkfile.name.lower().endswith(e):
                dataunpackdirectory = "%s-%s-%d" % (checkfile, bangsignatures.extensionprettyprint[e], 1)
                os.mkdir(dataunpackdirectory)

                # The result of the scan is a dictionary with the
                # following data, depending on the status of the scan
                # * the status of the scan (successful or not)
                # * the length of the data
                # * list of files that were unpacked, if any, plus
                #   labels for the unpacked files
                # * labels that were added, if any
                # * errors that were encountered, if any
                try:
                    # for these files the start offset of the
                    # files is always 0
                    unpackresult = bangsignatures.extensiontofunction[e](checkfile, 0, dataunpackdirectory, temporarydirectory)
                except AttributeError as ex:
                    os.rmdir(dataunpackdirectory)
                    continue

                if not unpackresult['status']:
                    # No data could be unpacked for some reason
                    # The unpack errror returned has more information:
                    #
                    # * offset in the file where the error occured
                    #   (integer)
                    # * error message (human readable)
                    # * flag to indicate if it is a fatal error
                    #   (boolean)

                    # log the file name, the extension and the
                    # error message for later analysis
                    logging.debug("FAIL %s known extension %s: %s" % (checkfile, e, unpackresult['error']['reason']))

                    # Fatal errors should lead to the program stopping
                    # execution. Ignored for now.
                    if unpackresult['error']['fatal']:
                        pass

                    # Remove the unpacking directory, including any data that
                    # might accidentily be there, so first change the
                    # permissions of all the files so they can be safely.
                    dirwalk = os.walk(dataunpackdirectory)

                    for direntries in dirwalk:
                        # make sure all subdirectories and files can
                        # be accessed and then removed.
                        for subdir in direntries[1]:
                            subdirname = os.path.join(direntries[0], subdir)
                            if not os.path.islink(subdirname):
                                os.chmod(subdirname,
                                         stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)
                        for filename in direntries[2]:
                            fullfilename = os.path.join(direntries[0], filename)
                            if not os.path.islink(fullfilename):
                                os.chmod(fullfilename,
                                         stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)
                    shutil.rmtree(dataunpackdirectory)
                    continue

                # the file could be unpacked successfully,
                # so log it as such.
                logging.info("SUCCESS %s %s at offset: 0, length: %d" % (checkfile, e, unpackresult['length']))

                # store the location of where the successfully
                # unpacked file ends (the offset is always 0  here).
                lastunpackedoffset = unpackresult['length']

                # store the range of the unpacked data
                unpackedrange.append((0, unpackresult['length']))

                # store any labels that were passed as a result and
                # add them to the current list of labels
                labels += copy.deepcopy(unpackresult['labels'])
                labels = list(set(labels))

                # if unpackedfilesandlabels is empty, then no files
                # were unpacked likely because the whole file was the
                # result and didn't contain any files (it was not a
                # container or compresed file)
                if len(unpackresult['filesandlabels']) == 0:
                    os.rmdir(dataunpackdirectory)

                # whole file has already been unpacked, so no need for
                # further scanning.
                if unpackresult['length'] == filesize:
                    needsunpacking = False

                # store lot of information about the unpacked files
                report = {}
                report['offset'] = 0
                report['extension'] = e
                report['type'] = bangsignatures.extensionprettyprint[e]
                report['size'] = unpackresult['length']
                report['files'] = []

                for un in unpackresult['filesandlabels']:
                    (unpackedfile, unpackedlabel) = un
                    # add the data, plus possibly any label
                    scanfilequeue.put((pathlib.Path(unpackedfile), unpackedlabel, checkfile, {}))
                    report['files'].append(unpackedfile[len(dataunpackdirectory)+1:])
                fileresult['unpackedfiles'].append(report)

        # continue scanning if needed by searching for known signatures
        if needsunpacking:
            # keep a counter per signature for the unpacking
            # directory names
            counterspersignature = {}

            # always open the file in binary mode
            scanfile = open(checkfile, 'rb')
            scanfile.seek(max(lastunpackedoffset, 0))

            # keep track of where in the file the starting
            # point for scanning for signatures is
            offsetinfile = scanfile.tell()
            scanbytesarray = bytearray(scanenvironment['maxbytes'])
            bytesread = scanfile.readinto(scanbytesarray)
            scanbytes = memoryview(scanbytesarray)

            # search the data for known signatures in the data that was read
            while True:
                candidateoffsetsfound = set()
                for s in bangsignatures.signatures:
                    res = re.finditer(re.escape(bangsignatures.signatures[s]), scanbytes[:bytesread])
                    if res is not None:
                        for r in res:
                            if s in bangsignatures.signaturesoffset:
                                # skip files that aren't big enough if the
                                # signature is not at the start of the data
                                # to be carved (example: ISO9660).
                                if r.start() + offsetinfile - bangsignatures.signaturesoffset[s] < 0:
                                    continue
                            offset = r.start()

                            # first perform a few sanity checks to prevent
                            # false positives for the built in unpack
                            # functions in BANG, as function calls are
                            # expensive so prevent them as much as possible.
                            # For big files this can easily save hundreds of
                            # thousands of function calls.
                            #
                            # Included here are checks for:
                            #
                            # * LZMA
                            # * bzip2
                            # * gzip
                            # * BMP
                            # * SGI images
                            # * ICO
                            # * PNG
                            # * MNG
                            # * TrueType and OpenType fonts
                            # * terminfo
                            if s in ['lzma_var1', 'lzma_var2', 'lzma_var3']:
                                # header of LZMA files is 13 bytes
                                if filesize - (offset + offsetinfile) < 13:
                                    continue
                                # Only do this if there are enough bytes
                                # left to test on, otherwise let the sliding
                                # window do its work
                                if bytesread - offset >= 13:
                                    # bytes 5 - 13 are the size field. It
                                    # could be that it is undefined, but if
                                    # it is defined then check if it is too
                                    # large or too small.
                                    if scanbytes[offset+5:offset+13] != b'\xff\xff\xff\xff\xff\xff\xff\xff':
                                        lzmaunpackedsize = int.from_bytes(scanbytes[offset+5:offset+13], byteorder='little')
                                        if lzmaunpackedsize == 0:
                                            continue

                                        # XZ Utils cannot unpack or create
                                        # files with size of 256 GiB or more
                                        if lzmaunpackedsize > 274877906944:
                                            continue
                            elif s == 'bzip2':
                                # first some sanity checks consisting of
                                # header checks:
                                #
                                # * block size
                                # * magic
                                #
                                # Only do this if there are enough bytes
                                # left to test on, otherwise
                                # let the sliding window do its work
                                if bytesread - offset >= 10:
                                    # the byte indicating the block size
                                    # has to be in the range 1 - 9
                                    try:
                                        blocksize = int(scanbytes[offset+3])
                                    except:
                                        continue
                                    # block size byte cannot be 0
                                    if blocksize == 0:
                                        continue
                                    # then check if the file is a stream or
                                    # not. If so, some more checks can be
                                    # made (bzip2 source code decompress.c,
                                    # line 224).
                                    if scanbytes[offset+4] != b'\x17':
                                        if scanbytes[offset+4:offset+10] != b'\x31\x41\x59\x26\x53\x59':
                                            continue
                            elif s == 'gzip':
                                # first some sanity checks consisting of
                                # header checks.
                                #
                                # RFC 1952 http://www.zlib.org/rfc-gzip.html
                                # describes the flags, but omits the
                                # "encrytion" flag (bit 5)
                                #
                                # Python 3's zlib module does not support:
                                # * continuation of multi-part gzip (bit 2)
                                # * encrypt (bit 5)
                                #
                                # RFC 1952 says that bit 6 and 7 should not
                                # be set.
                                if bytesread - offset >= 4:
                                    gzipbyte = scanbytes[offset+3]
                                    if (gzipbyte >> 2 & 1) == 1:
                                        # continuation of multi-part gzip
                                        continue
                                    if (gzipbyte >> 5 & 1) == 1:
                                        # encrypted
                                        continue
                                    if (gzipbyte >> 6 & 1) == 1:
                                        # reserved
                                        continue
                                    if (gzipbyte >> 7 & 1) == 1:
                                        # reserved
                                        continue
                            elif s == 'bmp':
                                # header of BMP files is 26 bytes
                                if filesize - (offset + offsetinfile) < 26:
                                    continue
                                if bytesread - offset >= 6:
                                    bmpsize = int.from_bytes(scanbytes[offset+2:offset+6], byteorder='little')
                                    if offsetinfile + offset + bmpsize > filesize:
                                        continue
                            elif s == 'sgi':
                                # header of SGI files is 512 bytes
                                if filesize - (offset + offsetinfile) < 512:
                                    continue
                                if bytesread - offset > 512:
                                    # storage format
                                    if not (scanbytes[offset+2] == 0 or scanbytes[offset+2] == 1):
                                        continue
                                    # BPC
                                    if not (scanbytes[offset+3] == 1 or scanbytes[offset+3] == 2):
                                        continue
                                    # dummy values, last 404 bytes of
                                    # the header are 0x00
                                    if not scanbytes[offset+108:offset+512] == b'\x00' * 404:
                                        continue
                            elif s == 'ico':
                                # check the number of images
                                if filesize - (offset + offsetinfile) < 22:
                                    continue
                                numberofimages = int.from_bytes(scanbytes[offset+4:offset+6], byteorder='little')
                                if numberofimages == 0:
                                    continue

                                # images cannot be outside of the file
                                if offsetinfile + offset + 6 + numberofimages * 16 > filesize:
                                    continue

                                # Then check the first image, as this
                                # is where most false positives happen.
                                imagesize = int.from_bytes(scanbytes[offset+14:offset+18], byteorder='little')
                                if imagesize == 0:
                                    continue

                                # ICO cannot be outside of the file
                                imageoffset = int.from_bytes(scanbytes[offset+18:offset+22], byteorder='little')

                                if offsetinfile + offset + imageoffset + imagesize > filesize:
                                    continue
                            elif s == 'png':
                                # minimum size of PNG files is 57 bytes
                                if filesize - (offsetinfile + offset) < 57:
                                    continue
                                if bytesread - offset >= 13:
                                    # bytes 8 - 11 are always the same in
                                    # every PNG
                                    if scanbytes[offset+8:offset+12] != b'\x00\x00\x00\x0d':
                                        continue
                            elif s == 'mng':
                                # minimum size of MNG files is 52 bytes
                                if filesize - (offsetinfile + offset) < 52:
                                    continue
                                if bytesread - offset >= 13:
                                    # bytes 8 - 11 are always the same in
                                    # every MNG
                                    if scanbytes[offset+8:offset+12] != b'\x00\x00\x00\x1c':
                                        continue
                            elif s == 'truetype' or s == 'opentype':
                                if filesize - (offsetinfile + offset) < 12:
                                    continue
                                # two simple sanity checks: number of
                                # tables and search range
                                numtables = int.from_bytes(scanbytes[offset+4:offset+6], byteorder='big')

                                if numtables == 0:
                                    continue

                                # then the search range
                                searchrange = int.from_bytes(scanbytes[offset+6:offset+8], byteorder='big')
                                if pow(2, int(math.log2(numtables)))*16 != searchrange:
                                    continue
                            elif s == 'terminfo':
                                if filesize - (offsetinfile + offset) < 12:
                                    continue

                                # simple sanity check: names section
                                # size cannot be < 2 or > 128
                                namessectionsize = int.from_bytes(scanbytes[offset+2:offset+4], byteorder='little')

                                if namessectionsize < 2 or namessectionsize > 128:
                                    continue

                            # default: store a tuple (offset, signature name)
                            if s in bangsignatures.signaturesoffset:
                                candidateoffsetsfound.add((offset + offsetinfile - bangsignatures.signaturesoffset[s], s))
                            else:
                                candidateoffsetsfound.add((offset + offsetinfile, s))

                # For each of the found candidates see if any
                # data can be unpacked. Process these in the order
                # in which the signatures were found in the file.
                for s in (sorted(candidateoffsetsfound)):
                    # skip offsets which are not useful to look at
                    # for example because the data has already been
                    # unpacked.
                    if s[0] < lastunpackedoffset:
                        continue

                    # first see if there actually is a method to unpack
                    # this type of file
                    if not s[1] in bangsignatures.signaturetofunction:
                        continue

                    # always change to the declared unpacking directory
                    os.chdir(unpackdirectory)

                    # then create an unpacking directory specifically
                    # for the signature including the signature name
                    # and a counter for the signature.
                    if not s[1] in counterspersignature:
                        namecounter = 1
                    else:
                        namecounter = counterspersignature[s[1]] + 1
                    while True:
                            dataunpackdirectory = "%s-%s-%d" % (checkfile, bangsignatures.signatureprettyprint.get(s[1], s[1]), namecounter)
                            try:
                                os.mkdir(dataunpackdirectory)
                                break
                            except:
                                namecounter += 1

                    # run the scan for the offset that was found
                    # First log which identifier was found and
                    # at which offset for possible later analysis.
                    logging.debug("TRYING %s %s at offset: %d" % (checkfile, s[1], s[0]))

                    # The result of the scan is a dictionary containing:
                    #
                    # * the status of the scan (successful or not)
                    #
                    # Successful scans also contain:
                    #
                    # * the length of the data
                    # * list of files that were unpacked, if any, plus
                    #   labels for the unpacked files
                    # * labels that were added, if any
                    #
                    # Unsucccesful scans contain:
                    #
                    # * errors that were encountered
                    try:
                        unpackresult = bangsignatures.signaturetofunction[s[1]](checkfile, s[0], dataunpackdirectory, temporarydirectory)
                    except AttributeError as e:
                        os.rmdir(dataunpackdirectory)
                        continue

                    if not unpackresult['status']:
                        # No data could be unpacked for some reason,
                        # so log the status and error message
                        logging.debug("FAIL %s %s at offset: %d: %s" % (checkfile, s[1], s[0], unpackresult['error']['reason']))

                        # unpacking error contains:
                        #
                        # * offset in the file where the error occured
                        #   (integer)
                        # * error message (human readable)
                        # * flag to indicate if it is a fatal error
                        #   (boolean)
                        #
                        # Fatal errors should lead to the program
                        # stopping execution. Ignored for now.
                        if unpackresult['error']['fatal']:
                            pass

                        # clean up any data that might have been left behind
                        # remove the directory, so first change the
                        # permissions of all the files so they can be
                        # safely.
                        dirwalk = os.walk(dataunpackdirectory)

                        for direntries in dirwalk:
                            # make sure all subdirectories and files
                            # can be accessed
                            for subdir in direntries[1]:
                                subdirname = os.path.join(direntries[0], subdir)
                                if not os.path.islink(subdirname):
                                    os.chmod(subdirname, stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)
                            for filename in direntries[2]:
                                fullfilename = os.path.join(direntries[0], filename)
                                if not os.path.islink(fullfilename):
                                    os.chmod(fullfilename, stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)

                        # finally remove the unpacking directory,
                        # and continue with the next candidate offset
                        shutil.rmtree(dataunpackdirectory)
                        continue

                    # the file could be unpacked successfully,
                    # so log it as such.
                    logging.info("SUCCESS %s %s at offset: %d, length: %d" % (checkfile, s[1], s[0], unpackresult['length']))

                    # store the name counter
                    counterspersignature[s[1]] = namecounter

                    # store the labels for files that could be
                    # unpacked/verified completely.
                    if s[0] == 0 and unpackresult['length'] == filesize:
                        labels += copy.deepcopy(unpackresult['labels'])
                        labels = list(set(labels))
                        # if unpackedfilesandlabels is empty, then no
                        # files were unpacked, likely because the whole
                        # file was the result and didn't contain any
                        # files (i.e. it was not a container file or
                        # compresed file).
                        if len(unpackresult['filesandlabels']) == 0:
                            os.rmdir(dataunpackdirectory)

                    # store the range of the unpacked data
                    unpackedrange.append((s[0], s[0] + unpackresult['length']))

                    # store lot of information about the unpacked files
                    report = {}
                    report['offset'] = s[0]
                    report['signature'] = s[1]
                    report['type'] = bangsignatures.signatureprettyprint.get(s[1], s[1])
                    report['size'] = unpackresult['length']
                    report['files'] = []

                    # set unpackdirectory, but only if needed: if the entire
                    # file is a file that was verified (example: GIF or PNG)
                    # then there will not be an unpacking directory.
                    if len(unpackresult['filesandlabels']) != 0:
                        report['unpackdirectory'] = dataunpackdirectory[lenunpackdirectory:]

                    for un in unpackresult['filesandlabels']:
                        (unpackedfile, unpackedlabel) = un

                        # TODO: make relative wrt unpackdir
                        report['files'].append(unpackedfile[len(dataunpackdirectory)+1:])

                        # add the data, plus possibly any label
                        scanfilequeue.put((pathlib.Path(unpackedfile), unpackedlabel, checkfile, {}))

                    fileresult['unpackedfiles'].append(report)

                    # skip over all of the indexes that are now known
                    # to be false positives
                    lastunpackedoffset = s[0] + unpackresult['length']

                    # something was unpacked, so record it as such
                    needsunpacking = False

                # check if the end of file has been reached, if so exit
                if scanfile.tell() == filesize:
                    break

                # see where to start reading next.
                if scanfile.tell() < lastunpackedoffset:
                    # If data has already been unpacked it can be skipped.
                    scanfile.seek(lastunpackedoffset)
                else:
                    # use an overlap
                    scanfile.seek(-maxsignaturesoffset, 1)
                offsetinfile = scanfile.tell()

                bytesread = scanfile.readinto(scanbytesarray)

            scanfile.close()

        # Now carve any data that was not unpacked from the file and
        # put it back into the scanning queue to see if something
        # could be unpacked after all.
        #
        # This also makes it easier for doing a "post mortem".
        if unpackedrange != []:
            # first check if the first entry covers the entire file
            # because if so there is nothing to do
            if not (unpackedrange[0][0] == 0 and unpackedrange[0][1] == filesize):
                synthesizedcounter = 1
                startoffset = 0
                scanfile = open(checkfile, 'rb')
                scanfile.seek(0)
                # then try to see if the any useful data can be uncarved.
                # Add an artifical entry for the end of the file
                for u in unpackedrange + [(filesize, filesize)]:
                    if u[0] > startoffset:
                        #if u[0] - startoffset < synthesizedminimum:
                        #        startoffset = u[1]
                        #        continue
                        dataunpackdirectory = "%s-%s-%d" % (checkfile, "synthesized", synthesizedcounter)
                        try:
                            os.mkdir(dataunpackdirectory)
                        except:
                            break
                        synthesizedcounter += 1

                        outfilename = os.path.join(dataunpackdirectory, "unpacked")
                        outfile = open(outfilename, 'wb')
                        os.sendfile(outfile.fileno(), scanfile.fileno(), startoffset, u[0] - startoffset)
                        outfile.close()
                        startoffset = u[1]+1

                        unpackedlabel = ['synthesized']

                        # try to see if the file contains NUL byte padding
                        # and if so tag it as such
                        ispadding = True
                        outfile = open(outfilename, 'rb')
                        checkbytes = outfile.read(1)
                        if checkbytes == b'\x00':
                            ## now read more bytes
                            while True:
                                scanbytes = outfile.read(scanenvironment['maxbytes'])
                                if len(scanbytes) == 0:
                                    break
                                if scanbytes != len(scanbytes) * b'\x00':
                                    ispadding = False
                                    break
                        else:
                            ispadding = False
                        outfile.close()
                        if ispadding:
                            unpackedlabel.append('padding')

                        # add the data, plus labels, to the queue
                        scanfilequeue.put((pathlib.Path(outfilename), unpackedlabel, checkfile, {}))
                    startoffset = u[1]
                scanfile.close()

        # no files were unpacked, so try to scan the entire file.
        # This happens for example with text only files like base64
        # or Intel hex (all ASCII only).
        if needsunpacking:
            if 'text' in labels and unpackedrange == []:
                # finally try all the "text only" functions
                for f in bangsignatures.textonlyfunctions:
                    namecounter = 1
                    dataunpackdirectory = "%s-%s-%d" % (checkfile, f, namecounter)
                    while True:
                        try:
                            os.mkdir(dataunpackdirectory)
                            break
                        except:
                            namecounter += 1

                    logging.debug("TRYING %s %s at offset: 0" % (checkfile, f))
                    try:
                        unpackresult = bangsignatures.textonlyfunctions[f](checkfile, 0, dataunpackdirectory, temporarydirectory)
                    except Exception as e:
                        os.rmdir(dataunpackdirectory)
                        continue

                    if not unpackresult['status']:
                        # No data could be unpacked for some reason,
                        # so check the status first
                        logging.debug("FAIL %s %s at offset: %d: %s" % (checkfile, f, 0, unpackresult['error']['reason']))
                        #print(s[1], unpackresult['error'])
                        #sys.stdout.flush()
                        # unpackerror contains:
                        # * offset in the file where the error occured
                        #   (integer)
                        # * reason of the error (human readable)
                        # * flag to indicate if it is a fatal error
                        #   (boolean)
                        #
                        # Fatal errors should stop execution of the
                        # program and remove the unpacking directory,
                        # so first change the permissions of
                        # all the files so they can be safely removed.
                        if unpackresult['error']['fatal']:
                            pass
                        # clean up any data that might have been left behind
                        dirwalk = os.walk(dataunpackdirectory)

                        for direntries in dirwalk:
                            # make sure all subdirectories and files
                            # can be accessed
                            for subdir in direntries[1]:
                                subdirname = os.path.join(direntries[0], subdir)
                                if not os.path.islink(subdirname):
                                    os.chmod(subdirname, stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)
                            for filename in direntries[2]:
                                fullfilename = os.path.join(direntries[0], filename)
                                if not os.path.islink(fullfilename):
                                    os.chmod(fullfilename, stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)
                        shutil.rmtree(dataunpackdirectory)
                        continue

                    logging.info("SUCCESS %s %s at offset: %d, length: %d" % (checkfile, f, 0, unpackresult['length']))

                    # store the labels for files that could be
                    # unpacked/verified completely.
                    if unpackresult['length'] == filesize:
                        labels += copy.deepcopy(unpackresult['labels'])
                        labels = list(set(labels))
                        # if unpackedfilesandlabels is empty, then no
                        # files were unpacked, likely because the whole
                        # file was the result and didn't contain any
                        # files (i.e. it was not a container file or
                        # compresed file).
                        if len(unpackresult['filesandlabels']) == 0:
                            os.rmdir(dataunpackdirectory)

                    # store lot of information about the unpacked files
                    report = {}
                    report['offset'] = 0
                    report['signature'] = f
                    report['type'] = f
                    report['size'] = unpackresult['length']
                    report['files'] = []

                    lastunpackedoffset = unpackresult['length']
                    unpackedrange.append((0, unpackresult['length']))

                    for un in unpackresult['filesandlabels']:
                        (unpackedfile, unpackedlabel) = un

                        # TODO: make relative wrt unpackdir
                        report['files'].append(unpackedfile[len(dataunpackdirectory)+1:])

                        # add the data, plus possibly any label
                        scanfilequeue.put((pathlib.Path(unpackedfile), unpackedlabel, checkfile, {}))

                    fileresult['unpackedfiles'].append(report)
                    break

        # restore the original unpacked range for files
        # that were rescanned.
        if 'rescan' in labels:
            unpackedrange = origunpackedrange
            labels.remove('rescan')

        # run individual scans for files and store them in a separate
        # result file, but only do this once for every file, so check
        # the shared dictionary with checksum results first.
        duplicate = False

        processlock.acquire()
        if hashresults['sha256'] in checksumdict:
            duplicate = True
        else:
            checksumdict[hashresults['sha256']] = checkfile
        processlock.release()

        if not duplicate:
            # Run scans for each individual file. Parameters include:
            # * filename (pathlib.Path object)
            # * hashes (dict 'hashresults')

            for b in bangfilefunctions:
                runscan = True
                ignorelist = set(b[1])
                filefunc = b[0]
                for label in labels:
                    if label in ignorelist:
                        runscan = False
                        break
                if runscan:
                    res = filefunc[1](checkfile, hashresults, dbconn, dbcursor, scanenvironment)

            # write a pickle with output data
            # The pickle contains:
            # * all available hashes
            # * labels
            # * byte count
            # * any extra data that might have been passed around
            resultout = {}
            if createbytecounter:
                resultout['bytecount'] = sorted(bytecounter.items())
                # write a file with the distribution of bytes in the scanned file
                bytescountfilename = resultsdirectory / ("%s.bytes" % hashresults['sha256'])
                if not bytescountfilename.exists():
                    bytesout = bytescountfilename.open('w')
                    for by in resultout['bytecount']:
                        bytesout.write("%d\t%d\n" % by)
                    bytesout.close()

            for i in hashresults:
                resultout[i] = hashresults[i]

            resultout['labels'] = labels
            if extradata != {}:
                resultout['extra'] = extradata
            picklefilename = resultsdirectory / ("%s.pickle" % hashresults['sha256'])
            if not picklefilename.exists():
                pickleout = picklefilename.open('wb')
                pickle.dump(resultout, pickleout)
                pickleout.close()

        else:
            labels.append('duplicate')

        fileresult['labels'] = list(set(labels))
        fileresult['filesize'] = filesize
        print(json.dumps(fileresult))
        sys.stdout.flush()

        resultqueue.put(fileresult)
        scanfilequeue.task_done()


def main(argv):
    # first parse the arguments provided to the script
    parser = argparse.ArgumentParser()
    parser.add_argument("-f", "--file", action="store", dest="checkfile",
                        help="path to file to check", metavar="FILE")
    parser.add_argument("-d", "--directory", action="store", dest="checkdirectory",
                        help="path to directory with files to check", metavar="DIR")
    parser.add_argument("-c", "--config", action="store", dest="cfg",
                        help="path to configuration file", metavar="FILE")
    args = parser.parse_args()

    if args.checkdirectory is not None and args.checkfile is not None:
        parser.error("Cannot scan a directory and a single file, exiting")

    # sanity checks for the configuration file
    if args.cfg is None:
        parser.error("No configuration file provided, exiting")

    # the configuration file should exist ...
    if not os.path.exists(args.cfg):
        parser.error("File %s does not exist, exiting." % args.cfg)

    # ... and should be a real file
    if not stat.S_ISREG(os.stat(args.cfg).st_mode):
        parser.error("%s is not a regular file, exiting." % args.cfg)

    # read the configuration file. This is in Windows INI format.
    config = configparser.ConfigParser()

    try:
        configfile = open(args.cfg, 'r')
        config.readfp(configfile)
    except:
        print("Cannot open configuration file, exiting", file=sys.stderr)
        sys.exit(1)

    # set a few default values that can be redefined
    # in the configuration file
    baseunpackdirectory = ''
    temporarydirectory = None
    removescandirectory = False

    # some default values for the database, possibly
    # overridden in the configuration file
    postgresql_host = None
    postgresql_port = None
    usedatabase = True
    dbconnectionerrorfatal = False

    # then process each individual section and extract
    # configuration options
    for section in config.sections():
        if section == 'configuration':
            # The base unpack directory is where the unpacked files
            # will be written. This is mandatory.
            try:
                baseunpackdirectory = config.get(section, 'baseunpackdirectory')
            except Exception:
                break
            # The temporary directory is where temporary files will be
            # written.  This is optional. If not set the system's
            # temporary directory (usually /tmp ) will be used.
            try:
                temporarydirectory = config.get(section, 'temporarydirectory')
            except Exception:
                pass

            # The number of threads to be created to scan the files
            # recursively, next to the main thread. Defaults to "all
            # availabe threads" (number of CPUs on a machine).
            try:
                bangthreads = min(int(config.get(section, 'threads')), multiprocessing.cpu_count())
                # if 0 or a negative number was configured, then use
                # all available threads
                if bangthreads < 1:
                    bangthreads = multiprocessing.cpu_count()
            except Exception:
                # use all available threads by default
                bangthreads = multiprocessing.cpu_count()
            try:
                remove = config.get(section, 'removescandirectory')
                if remove == 'yes':
                    removescandirectory = True
            except Exception:
                pass

        elif section == 'database':
            # The default mode is to continue without having a database
            # and to just disable the functionality that requires a
            # database, but this makes it harder to detect that a
            # database might be down.
            try:
                connectionerrorfatal = config.get(section,
                                                  'dbconnectionerrorfatal')
                if connectionerrorfatal == 'yes':
                    dbconnectionerrorfatal = True
            except:
                pass

            # Extract the minimally necessary information for
            # PostgreSQL to be able to connect.
            try:
                postgresql_user = config.get(section, 'postgresql_user')
                postgresql_password = config.get(section, 'postgresql_password')
                postgresql_db = config.get(section, 'postgresql_db')
            except:
                usedatabase = False
                if dbconnectionerrorfatal:
                    print("Configuration file malformed: missing or wrong database information", file=sys.stderr)
                    configfile.close()
                    sys.exit(1)

            # Extract some extra, optional, connection information
            try:
                postgresql_host = config.get(section, 'postgresql_host')
            except:
                pass
            try:
                postgresql_port = config.get(section, 'postgresql_port')
            except:
                pass

    configfile.close()

    # test the database. If the connection fails it depends on whether
    # or not it should be treated as a fatal error. If not, then
    # continue without using any of the database functionality.
    try:
        c = psycopg2.connect(database=postgresql_db, user=postgresql_user,
                             password=postgresql_password,
                             port=postgresql_port, host=postgresql_host)
        c.close()
    except Exception as e:
        if dbconnectionerrorfatal:
            print("Configuration file malformed: missing or wrong database information", file=sys.stderr)
            configfile.close()
            sys.exit(1)
        usedatabase = False

    # tuple of database connection/database cursor
    bangdbconns = []

    # create database connections if any database is used
    if usedatabase:
        for i in range(0, bangthreads):
            bangconn = psycopg2.connect(database=postgresql_db,
                                        user=postgresql_user,
                                        password=postgresql_password,
                                        port=postgresql_port,
                                        host=postgresql_host)
            bangcursor = bangconn.cursor()
            bangdbconns.append((bangconn, bangcursor))

    # Check if the base unpack directory was declared.
    if baseunpackdirectory == '':
        print("Base unpack directory not declared in configuration file, exiting", file=sys.stderr)
        sys.exit(1)

    # Check if the base unpack directory exists
    if not os.path.exists(baseunpackdirectory):
        print("Base unpack directory %s does not exist, exiting" % baseunpackdirectory, file=sys.stderr)
        sys.exit(1)

    if not os.path.isdir(baseunpackdirectory):
        print("Base unpack directory %s is not a directory, exiting" % baseunpackdirectory, file=sys.stderr)
        sys.exit(1)

    # Check if the base unpack directory can be written to
    try:
        testfile = tempfile.mkstemp(dir=baseunpackdirectory)
        os.unlink(testfile[1])
    except:
        print("Base unpack directory %s cannot be written to, exiting" % baseunpackdirectory, file=sys.stderr)
        sys.exit(1)

    # Check if the temporary directory is actually an existing
    # directory, but only if it was defined in the configuration file.
    if temporarydirectory is not None:
        if not os.path.exists(temporarydirectory):
            print("Temporary directory %s does not exist, exiting" % temporarydirectory, file=sys.stderr)
            sys.exit(1)

        if not os.path.isdir(temporarydirectory):
            print("Temporary directory %s is not a directory, exiting" % temporarydirectory, file=sys.stderr)
            sys.exit(1)

        # Check if the temporary directory can be written to
        try:
            testfile = tempfile.mkstemp(dir=temporarydirectory)
            os.unlink(testfile[1])
        except:
            print("Temporary directory %s cannot be written to, exiting" % temporarydirectory, file=sys.stderr)
            sys.exit(1)

    # first determine how many bytes should be scanned for known
    # signatures using a sliding window. This should not be set too
    # large for performance reasons and not too low (to avoid a
    # silly window). Ideally this is a few times the value of
    # 'maxsignaturesoffset'
    maxbytes = max(200000, maxsignaturesoffset+1)

    # grab all the functions from the "bangfilescans" file
    bangfunctions = inspect.getmembers(bangfilescans, inspect.isfunction)

    # split them functions in file functions and others
    bangfilefunctions = []
    bangwholecontextfunctions = []

    for f in bangfunctions:
        context = None
        ignorelist = []
        if f[1].__doc__ is None:
            bangfilefunctions.append((f, []))
        else:
            for l in f[1].__doc__.split('\n'):
                if l.strip().startswith('Context:'):
                    context = l.split(':', 1)[1].strip()
                elif l.strip().startswith('Ignore:'):
                    ignoretypes = l.strip().split(',')
                    for i in ignoretypes:
                        ignorelist.append(i.strip())
            if context == 'file':
                bangfilefunctions.append((f, ignorelist))
            elif context == 'whole':
                bangwholecontextfunctions.append((f, ignorelist))

    checkfiles = []
    if args.checkdirectory is not None:
        # should be a directory
        if not stat.S_ISDIR(os.stat(args.checkdirectory).st_mode):
            parser.error("%s is not a directory, exiting." % args.checkdirectory)

        dirwalk = os.walk(args.checkdirectory)
        for i in dirwalk:
            for j in i[2]:
                scanfilename = os.path.join(i[0], j)
                if not os.path.exists(scanfilename):
                    continue

                # ... and should be a real file
                if not stat.S_ISREG(os.stat(scanfilename).st_mode):
                    continue

                filesize = os.stat(scanfilename).st_size
                # Don't scan an empty file
                if filesize == 0:
                    continue
                checkfiles.append(scanfilename)
    else:
        # sanity checks for the file to scan
        if args.checkfile is None:
            parser.error("No file to scan provided, exiting")

        # the file to scan should exist ...
        if not os.path.exists(args.checkfile):
            parser.error("File %s does not exist, exiting." % args.checkfile)

        # ... and should be a real file
        if not stat.S_ISREG(os.stat(args.checkfile).st_mode):
            parser.error("%s is not a regular file, exiting." % args.checkfile)

        filesize = os.stat(args.checkfile).st_size

        # Don't scan an empty file
        if filesize == 0:
            print("File to scan is empty, exiting", file=sys.stderr)
            sys.exit(1)

        checkfiles.append(args.checkfile)

    if len(checkfiles) == 0:
        print("No files to scan found, exiting", file=sys.stderr)
        sys.exit(1)

    # set up logging
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(message)s')
    banglogger = logging.getLogger()

    # remove the standard log handler, as it will be different
    # per scan directory
    for i in banglogger.handlers:
        banglogger.removeHandler(i)

    for checkfile in sorted(checkfiles):
        # Now the real scanning starts.
        scandate = datetime.datetime.utcnow()

        # create a directory for the scan
        scandirectory = pathlib.Path(tempfile.mkdtemp(prefix='bang-scan-',
                                                  dir=baseunpackdirectory))

        # create an empty file "STARTED" to easily identify
        # active (or crashed) scans.
        startedfile = open(scandirectory / "STARTED", 'wb')
        startedfile.close()

        # now create a directory structure inside the scandirectory:
        # unpack/ -- this is where all the unpacked data will be stored
        # results/ -- this is where files describing the unpacked data
        #             will be stored
        # logs/ -- this is where logs from the scan will be stored
        unpackdirectory = scandirectory / "unpack"
        unpackdirectory.mkdir()

        resultsdirectory = scandirectory / "results"
        resultsdirectory.mkdir()

        logdirectory = scandirectory / "logs"
        logdirectory.mkdir()

        # create a log file inside the log directory and
        # add it to the BANG logger
        bangloghandler = logging.FileHandler(filename=logdirectory / 'unpack.log')
        banglogger.addHandler(bangloghandler)
        logging.info("Started scanning %s" % checkfile)

        processmanager = multiprocessing.Manager()

        # first create two queues: one for scanning files, the other one
        # for reporting results.
        scanfilequeue = processmanager.JoinableQueue(maxsize=0)
        resultqueue = processmanager.JoinableQueue(maxsize=0)
        processes = []

        # copy the file that needs to be scanned to the temporary
        # directory.
        try:
            shutil.copy(checkfile, unpackdirectory)
        except:
            print("Could not copy %s to scanning directory %s" % (checkfile, unpackdirectory), file=sys.stderr)
            logging.warning("Could not copy %s to scanning directory" % checkfile)
            logging.info("Finished scanning %s" % checkfile)
            # move the file "STARTED" to "FINISHED" to easily identify
            # active (or crashed) scans
            shutil.move(scandirectory / "STARTED",
                        scandirectory / "FINISHED")
            os.utime(scandirectory / "FINISHED")

            if removescandirectory:
                shutil.rmtree(scandirectory)
            continue

        # The scan queue will be used to put files into that need to be
        # scanned and processes. New files wil keep being added to it
        # while results are being unpacked recursively.
        # Initially one file will be in this queue, namely the first file.
        # After files are unpacked they will be added to the queue, as they
        # can be scanned in a trivially parallel way.

        # Create a list of labels to pass around. The first element is
        # tagged as 'root', as it is the root of the unpacking tree.
        labels = ['root']
        scanfilequeue.put((unpackdirectory / os.path.basename(checkfile), labels, None, {}))

        # create a lock to control access to any shared data structures
        processlock = multiprocessing.Lock()

        # create a shared dictionary
        checksumdict = processmanager.dict()

        # create a scan environment for the new scan,
        # initially an empty dictionary
        scanenvironment = {}

        # set the maximum size for the amount of bytes to be read
        scanenvironment['maxbytes'] = maxbytes

        # set the size of bytes to be read during scanning hashes
        scanenvironment['readsize'] = 10240
        scanenvironment['createbytecounter'] = True
        scanenvironment['tlshmaximum'] = sys.maxsize

        # create processes for unpacking archives
        for i in range(0, bangthreads):
            if not usedatabase:
                p = multiprocessing.Process(target=processfile,
                                            args=(scanfilequeue,
                                                  resultqueue,
                                                  processlock,
                                                  checksumdict,
                                                  unpackdirectory,
                                                  resultsdirectory,
                                                  temporarydirectory,
                                                  None, None,
                                                  bangfilefunctions,
                                                  scanenvironment))
            else:
                p = multiprocessing.Process(target=processfile,
                                            args=(scanfilequeue,
                                                  resultqueue,
                                                  processlock,
                                                  checksumdict,
                                                  unpackdirectory,
                                                  resultsdirectory,
                                                  temporarydirectory,
                                                  bangdbconns[i][0],
                                                  bangdbconns[i][1],
                                                  bangfilefunctions,
                                                  scanenvironment))
            processes.append(p)

        # then start all the processes
        for p in processes:
            p.start()

        # wait for the queues to be empty.
        scanfilequeue.join()

        # There is one result for each file in the result
        # queue, which need to be merged into a structure
        # matching the directory tree that was unpacked. The name
        # of each file that is unpacked serves as key into
        # the structure.
        scantree = {}

        while True:
            try:
                fileresult = resultqueue.get_nowait()
                if 'filename' in fileresult:
                    scantree[fileresult['filename']] = copy.deepcopy(fileresult)
                    resultqueue.task_done()
            except queue.Empty as e:
                # Queue is empty
                break

        resultqueue.join()

        # Done processing, terminate processes that were created
        for p in processes:
            p.terminate()

        scandatefinished = datetime.datetime.utcnow()

        # move the file "STARTED" to "FINISHED" to easily identify
        # active (or crashed) scans
        shutil.move(scandirectory / "STARTED",
                    scandirectory / "FINISHED")
        os.utime(scandirectory / "FINISHED")

        # now store the scan tree results with other data
        scanresult = {}
        scanresult['scantree'] = scantree

        # statistics about this particular session
        scanresult['session'] = {}
        scanresult['session']['start'] = scandate
        scanresult['session']['stop'] = scandatefinished
        scanresult['session']['duration'] = (scandatefinished - scandate).total_seconds()
        scanresult['session']['user'] = getpass.getuser()

        # some information about the platform
        scanresult['platform'] = {}
        scanresult['platform']['machine'] = platform.machine()
        scanresult['platform']['architecture'] = platform.architecture()[0]
        scanresult['platform']['processor'] = platform.processor()
        scanresult['platform']['node'] = platform.node()
        scanresult['platform']['system'] = platform.system()
        scanresult['platform']['release'] = platform.release()
        scanresult['platform']['libc'] = platform.libc_ver()[0]
        scanresult['platform']['libcversion'] = platform.libc_ver()[1]

        # some information about the used Python version
        scanresult['python'] = {}
        scanresult['python']['version'] = platform.python_version()
        scanresult['python']['implementation'] = platform.python_implementation()

        # write all results to a Python pickle
        picklefile = open(scandirectory / 'bang.pickle', 'wb')
        pickle.dump(scanresult, picklefile)
        picklefile.close()

        # create a human readable report
        reportfile = open(scandirectory / 'report.txt', 'w')
        filenames = sorted(scanresult['scantree'].keys())
        for i in filenames:
            print("File: %s" % i, file=reportfile)
            print("=" * (6 + len(i)), file=reportfile)
            print(file=reportfile)
            if 'md5' in scanresult['scantree'][i]:
                print("MD5:", scanresult['scantree'][i]['md5'], file=reportfile)
                print("SHA256:", scanresult['scantree'][i]['sha256'], file=reportfile)
            if 'filesize' in scanresult['scantree'][i]:
                reportfilesize = scanresult['scantree'][i]['filesize']
                print("Size:", reportfilesize, file=reportfile)
            if 'mimetype' in scanresult['scantree'][i]:
                mimetype = scanresult['scantree'][i]['mimetype']
                print("MIME type:", mimetype, file=reportfile)
            if 'parent' in scanresult['scantree'][i]:
                print("Parent: %s" % scanresult['scantree'][i]['parent'],  file=reportfile)
            reportlabels = functools.reduce(lambda x, y: x + ", " + y, sorted(scanresult['scantree'][i]['labels']))
            print("Labels: %s" % reportlabels, file=reportfile)
            bytesscanned = 0
            if 'unpackedfiles' in scanresult['scantree'][i]:
                for l in scanresult['scantree'][i]['unpackedfiles']:
                    bytesscanned += l['size']
                print("Bytes identified: %d (%f %%)" % (bytesscanned, bytesscanned/reportfilesize * 100), file=reportfile)
                for l in scanresult['scantree'][i]['unpackedfiles']:
                    if len(l['files']) != 0:
                        print("Data unpacked at offset %d from %s:" % (l['offset'], l['type']), file=reportfile)
                        filesunpacked = []
                        for f in l['files']:
                            filesunpacked.append(f)
                        if filesunpacked != []:
                            for f in sorted(filesunpacked):
                                print(" %s" % f, file=reportfile)
            print(file=reportfile)
            reportfile.flush()
        reportfile.close()

        # The end.
        logging.info("Finished scanning %s" % checkfile)

        # remove the log file from the system logger
        banglogger.removeHandler(bangloghandler)

        if removescandirectory:
            shutil.rmtree(scandirectory)

    # clean up the database cursors and
    # close all connections to the database
    for c in bangdbconns:
        # first close the cursor
        c[1].close()
        # then close the database connection
        c[0].close()


if __name__ == "__main__":
    main(sys.argv)
